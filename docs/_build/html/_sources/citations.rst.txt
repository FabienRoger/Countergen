Work We Use
------------------------------------

- LLMD `(Fryer, 2022) <https://aclanthology.org/2022.woah-1.20.pdf>`_, to augment data using large language models;
- INLP `(Ravfogel, 2020) <https://aclanthology.org/2020.acl-main.647/>`_ and RLACE `(Ravfogel, 2022) <https://arxiv.org/pdf/2201.12091.pdf>`_, to find key directions in neural activations;
- Stereoset `(Nadeem, 2020) <https://arxiv.org/abs/2004.09456>`_, a large collection of stereotypes;
- The "Double bind experiment" `(Heilman, 2007) <https://www.researchgate.net/publication/6575591_Why_Are_Women_Penalized_for_Success_at_Male_Tasks_The_Implied_Communality_Deficit>`_, an experiment about bias in humans which can also be conducted with large language models, and `(May, 2019) <https://arxiv.org/abs/1903.10561>`_, which provides the exact data we use;
- `OpenAI's API <https://openai.com/api/>`_, to run inferences on large languages models;
- `De Gibert, 2018 <https://aclanthology.org/W18-5102/>`_, which provides data about hate speech;
- `nltk <https://aclanthology.org/2022.woah-1.20.pdf>`_, and JÃ¶rg Michael's `gender.c <https://www.autohotkey.com/board/topic/20260-gender-verification-by-forename-cmd-line-tool-db/>`_, which contain datasets about the gender and origin of first names;
- BigBench's `Social Bias from Sentence Probability <https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/bias_from_probabilities>`_, which provides evaluation data and metrics.